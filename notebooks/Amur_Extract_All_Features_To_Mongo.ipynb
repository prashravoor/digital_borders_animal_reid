{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymongo as pm\n",
    "import cv2\n",
    "import cv2.dnn as dnn\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from bson.binary import Binary\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDB(client, dbName):\n",
    "    return client[dbName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCollection(client, dbName, collName):\n",
    "    return client[dbName][collName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializeDB(client, dbName):\n",
    "    db = getDB(client, dbName)\n",
    "    if len(db.list_collection_names()) > 0:\n",
    "        char = input('DB {} already contains some records. Reomve them? (y/n)'.format(dbName))\n",
    "        if char.lower()[0] == 'y':\n",
    "            for col in db.list_collection_names():\n",
    "                getCollection(client, dbName, col).drop()            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCaffeModel(modelName):\n",
    "    modelCfg = 'models/{}.prototxt'.format(modelName)\n",
    "    modelWeights = 'models/{}.caffemodel'.format(modelName)\n",
    "    return dnn.readNetFromCaffe(modelCfg, modelWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImageFromFile(filename):\n",
    "    return cv2.imread(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createBlobFromImage(image, newSize, scale=(1.0/255), meanSubtract=(0,0,0), swapRB=True, crop=False):\n",
    "    return dnn.blobFromImage(image, scale, newSize, meanSubtract, swapRB=swapRB, crop=crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNetOutputs(net, inputBlob, layerNames):\n",
    "    net.setInput(inputBlob)\n",
    "    return net.forward(layerNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRecord(feature, animalId, imageFile):\n",
    "    rec = {}\n",
    "    rec['animalId'] = animalId\n",
    "    rec['origSize'] = feature.shape\n",
    "    rec['imageFile'] = imageFile\n",
    "    rec['feature'] = Binary( pickle.dumps( feature, protocol=2) )\n",
    "    \n",
    "    return rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertRecords(client, dbName, docNames, records, features):\n",
    "    db = getDB(client, dbName)\n",
    "    for i in range(len(docNames)):\n",
    "        db[docNames[i]].insert_one(records[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertBatchToDB(client, dbName, docName, recs):\n",
    "    getCollection(client, dbName, docName).insert_many(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractFeaturesForImage(net, modelName, layerNames, imageFile):\n",
    "    inputSize = (227,227)\n",
    "    if 'googlenet' == modelName:\n",
    "        inputSize = (224,224)\n",
    "    elif 'resnet50' == modelName:\n",
    "        inputSize = (256,256)\n",
    "    \n",
    "    image = readImageFromFile(imageFile)\n",
    "    blob = createBlobFromImage(image, inputSize)\n",
    "    return getNetOutputs(net, blob, layerNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processImagesAndStoreFeatures(client, dbName, modelName, modelsDict, imagesList, idsList):\n",
    "    network = modelsDict[modelName]\n",
    "    layerNames = network.getLayerNames()\n",
    "    for i in range(len(imagesList)):\n",
    "        start = time.time()\n",
    "        features = extractFeaturesForImage(network, modelName, layerNames, imagesList[i])\n",
    "        print('Time for image classification is {}'.format(time.time() - start))\n",
    "        records = []\n",
    "        for feat in features:\n",
    "            records.append(createRecord(feat, idsList[i], imagesList[i]))\n",
    "        # Store Batch to DB - DB name is the model name + dataset name, and the collection is the layer name\n",
    "        start = time.time()\n",
    "        insertRecords(client, dbName + '_' + modelName, layerNames, records, features)\n",
    "        print('Time to store records is {}'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readIdsForImages(imagesList, setType, keyfile):\n",
    "    ids = []\n",
    "    mapping = {}\n",
    "    with open(keyfile) as f:\n",
    "        idpairs = list(map(lambda x: x.strip().split('\\t'), f.readlines()))\n",
    "        f.close()\n",
    "    \n",
    "    for pair in idpairs:\n",
    "        mapping[pair[0]] = int(pair[1])\n",
    "        \n",
    "    if setType == 'amur':\n",
    "        for image in imagesList:\n",
    "            base = os.path.basename(image)\n",
    "            ids.append(mapping[(base.split('.')[0])])\n",
    "    else:\n",
    "        for image in imagesList:\n",
    "            base = os.path.basename(image)\n",
    "            ids.append(mapping[base.split('_')[0]])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if len(args) < 1:\n",
    "        print('At least 1 images folder required')\n",
    "        return\n",
    "    client = pm.MongoClient('localhost', 27017)\n",
    "    imageFolders = args\n",
    "\n",
    "    # modelNames = ['alexnet', 'googlenet', 'resnet50']\n",
    "    modelNames = ['alexnet', 'googlenet']\n",
    "    modelsDict = {}\n",
    "    for model in modelNames:\n",
    "        modelsDict[model] = readCaffeModel(model)\n",
    "        print('Loaded Model {}'.format(model))\n",
    "    \n",
    "    for i in range(len(imageFolders)):\n",
    "        # Read Images from folder\n",
    "        path = imageFolders[i]\n",
    "        # Don't recurse into sub-folders, so get only reg files\n",
    "        files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "        # Get only JPGs\n",
    "        imageList = list(filter(lambda x: os.path.splitext(x)[1] == '.jpg', files))\n",
    "        imageList = list(map(lambda x: os.path.join(path, x), imageList))\n",
    "        if 'amur' in path.lower():\n",
    "            imType = 'amur'\n",
    "        else:\n",
    "            imType = 'elp'\n",
    "            \n",
    "        keyfile = path + '/class_mapping.txt'\n",
    "        if not os.path.exists(keyfile):\n",
    "            print('No KeyFile (class_mapping.txt) for folder {}, skipping!'.format(path))\n",
    "            continue\n",
    "\n",
    "        imageIds = readIdsForImages(imageList, imType, keyfile)\n",
    "        print('Found {} images in folder {}, Using Image Type {}'.format(len(imageList), path, imType))\n",
    "\n",
    "        for modelName in modelNames:\n",
    "            initializeDB(client, imType + '_' + modelName)\n",
    "            print('Processing {} images using Model {} for Set Type {}'.format(len(imageList), modelName, imType))\n",
    "            processImagesAndStoreFeatures(client, imType, modelName, modelsDict, imageList, imageIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main(['amur_small'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
